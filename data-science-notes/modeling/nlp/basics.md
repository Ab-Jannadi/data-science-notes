# Basics
## Word Tokenization
Word **Tokenization** also called as word segmentation is the process of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider.<br>
However, we still can have problems if we only split by space to achieve the wanted results. Some English compound nouns are variably written and sometimes they contain a space. Mostly making use of a library to achieve the wanted results, is a good idea.
### For example:
>San Francisco is a beautiful city. 

In the above sentence we can clearly see that if we consider `San` and `Francisco` as two different words then it totally changes the meaning of the word. 
## Sentence Tokenization
#
#
#
#

## Stemming and Text Lemmatization
#
#
#
#

## Stop Words
#
#
#
#

## Bag-of-Words
#
#
#
#

## Regex
#
#
#
#

## TF-IDF
#
#
#
#
